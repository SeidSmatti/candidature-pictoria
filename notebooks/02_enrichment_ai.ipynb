{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrichissement Sémantique d'un Corpus Visuel\n",
    "\n",
    "**Contexte :** Ce notebook présente un protocole de traitement pour l'analyse sémantique d'un corpus d'images numériques. L'objectif est d'appliquer des méthodes d'intelligence artificielle pour extraire automatiquement des mots-clés (tags) descriptifs, transformant ainsi une collection d'images brutes en une base de données structurée et interrogeable.\n",
    "\n",
    "**Démarche :** La méthodologie employée ici s'inscrit pleinement dans les pratiques des **Humanités Numériques**. Elle vise à combiner la rigueur technique avec une réflexion critique sur les outils, en vue de répondre à des problématiques de recherche en SHS. Nous utilisons un modèle de classification d'images \"Zero-Shot\", qui permet une grande flexibilité sans nécessiter de ré-entraînement coûteux, une approche pertinente issue de la **veille scientifique et technologique**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 1 : Initialisation de l'Environnement\n",
    "\n",
    "Nous commençons par importer les bibliothèques Python nécessaires et définir les chemins d'accès à notre corpus et aux fichiers de sortie. Cette étape garantit la **reproductibilité** de l'expérience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "from IPython.display import display\n",
    "\n",
    "# Configuration des chemins (relatifs à la racine du projet)\n",
    "CORPUS_PATH = Path('../corpus/images/')\n",
    "OUTPUT_JSON_PATH = Path('../corpus/ai_results.json')\n",
    "MODEL_NAME = \"openai/clip-vit-large-patch14\"\n",
    "\n",
    "print(\"Environnement configuré.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 2 : Définition du Cadre d'Analyse Sémantique\n",
    "\n",
    "Le choix des catégories est une étape méthodologique cruciale. Plutôt que d'utiliser des classes génériques, nous définissons ici une liste de labels spécifiquement adaptés à la nature de notre corpus (gravures techniques, photographies, etc.).\n",
    "\n",
    "Dans un projet de recherche réel, cette **élaboration de protocole** serait menée en collaboration avec les experts du domaine (historiens, sociologues, historiens de l'art) pour s'assurer de la pertinence des axes d'analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste de labels optimisée pour le corpus \"machines\"\n",
    "candidate_labels = [\n",
    "    \"schéma technique\",\n",
    "    \"gravure de paysage\",\n",
    "    \"scène de travail\",\n",
    "    \"photographie industrielle\",\n",
    "    \"document imprimé\",\n",
    "    \"scène historique ou allégorique\",\n",
    "    \"illustration\"\n",
    "]\n",
    "\n",
    "print(\"Cadre d'analyse défini avec les catégories suivantes :\")\n",
    "for label in candidate_labels:\n",
    "    print(f\"- {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 3 : Instanciation du Modèle d'IA\n",
    "\n",
    "Nous chargeons le pipeline de classification \"Zero-Shot\" de la bibliothèque Hugging Face. Le modèle choisi, CLIP, a été entraîné par OpenAI pour comprendre à la fois le texte et les images, ce qui lui permet de classer une image selon nos labels textuels sans avoir été spécifiquement entraîné pour cela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initialisation du modèle d'IA (CLIP)... Cette opération peut prendre quelques minutes lors du premier lancement.\")\n",
    "\n",
    "# Instanciation du pipeline. Le modèle sera téléchargé si nécessaire.\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-image-classification\",\n",
    "    model=MODEL_NAME,\n",
    "    device=\"cpu\" # Assure la compatibilité sur toutes les machines\n",
    ")\n",
    "\n",
    "print(\"Modèle prêt à l'emploi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 4 : Exécution du Protocole d'Analyse\n",
    "\n",
    "Le script parcourt maintenant chaque image du corpus. Pour chacune, il exécute l'inférence du modèle et stocke les résultats (la liste des labels classés par score de confiance). Le processus est transparent, affichant sa progression en temps réel. Cette approche garantit la **traçabilité** et la **reproductibilité** du traitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = sorted(list(CORPUS_PATH.glob('*.jpg')))\n",
    "results = {}\n",
    "total_images = len(image_files)\n",
    "\n",
    "print(f\"Début de l'analyse de {total_images} images...\")\n",
    "\n",
    "for i, image_path in enumerate(image_files):\n",
    "    try:\n",
    "        # Conversion en RGB pour assurer la compatibilité\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Inférence du modèle\n",
    "        predictions = classifier(image, candidate_labels=candidate_labels)\n",
    "        \n",
    "        # Stockage des résultats structurés\n",
    "        results[image_path.name] = {\n",
    "            'labels': [p['label'] for p in predictions],\n",
    "            'scores': [round(p['score'], 4) for p in predictions]\n",
    "        }\n",
    "        print(f\"  [{i+1}/{total_images}] Analyse de {image_path.name}... OK. (Top label: '{results[image_path.name]['labels'][0]}')\")\n",
    "    except Exception as e:\n",
    "        print(f\"    /!\\\\ Erreur lors de l'analyse de {image_path.name}: {e}\")\n",
    "        results[image_path.name] = {'error': str(e)}\n",
    "\n",
    "print(\"\\nAnalyse du corpus terminée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 5 : Sauvegarde et Valorisation des Données\n",
    "\n",
    "L'enrichissement est terminé. Nous sauvegardons les résultats dans un fichier JSON. Ce fichier structuré est un **livrable** essentiel : il ne contient plus seulement des chemins vers des images, mais des données exploitables.\n",
    "\n",
    "Ces données peuvent maintenant être intégrées dans des systèmes de gestion de bases de données, ou être utilisées pour générer des manifestes au format **IIIF**, ouvrant la voie à la création de **plateformes d'exploration** et de visualisation avancées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des résultats dans un fichier JSON\n",
    "with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Les données enrichies ont été sauvegardées dans : {OUTPUT_JSON_PATH}\")\n",
    "\n",
    "# Affichons un exemple pour visualiser le résultat\n",
    "example_image_name = '07.jpg'\n",
    "example_image_path = CORPUS_PATH / example_image_name\n",
    "\n",
    "print(f\"\\n--- Exemple de résultat pour l'image '{example_image_name}' ---\")\n",
    "\n",
    "# Affichage de l'image\n",
    "display(Image.open(example_image_path).resize((400, 300)))\n",
    "\n",
    "# Affichage des données JSON correspondantes\n",
    "print(json.dumps(results.get(example_image_name, {}), indent=2, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
